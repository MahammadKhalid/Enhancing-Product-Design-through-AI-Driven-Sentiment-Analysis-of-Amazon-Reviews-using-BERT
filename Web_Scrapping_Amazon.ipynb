{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6a55b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fake-useragent in c:\\users\\khali\\anaconda3\\envs\\sentiment\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: importlib-resources>=5.0 in c:\\users\\khali\\anaconda3\\envs\\sentiment\\lib\\site-packages (from fake-useragent) (5.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\khali\\anaconda3\\envs\\sentiment\\lib\\site-packages (from importlib-resources>=5.0->fake-useragent) (3.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fake-useragent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c233548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the keyword to search: B07DSZ3RQB\n",
      "B07DSZ3RQB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a858f9627148f2b0a261dff1a51215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from random import choice\n",
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "\n",
    "# to ignore SSL certificate errors\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "# random user-agent\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    'authority': 'www.amazon.com',\n",
    "    'pragma': 'no-cache',\n",
    "    'cache-control': 'no-cache',\n",
    "    'rtt': '50',\n",
    "    'downlink': '6.65',\n",
    "    'ect': '4g',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'referer': 'https://www.amazon.com/Heat-Storm-HS-1500-PHX-WIFI-Infrared-Heater/dp/B07JXRWJ8D/ref=cm_cr_arp_d_pl_foot_top?ie=UTF8&th=1',\n",
    "    'accept-language': 'en-US,en;q=0.9,la;q=0.8',\n",
    "    'cookie': 'ubid-main=133-9061931-0297943; sid=\"u39jVVzRWP15jSjEJsTdwA==|9ucGRH1SUgtb9e1e982gkm5/DayNcyYRTYXgNPZNa+g=\"; x-main=\"mE3y@VSbrs7MHrVVNnP1GN@GVZOaUJqe\"; at-main=Atza|IwEBIEv12ButU1VeMnrsclL17GM9BJ-80JPgG6TYbBZbAXZckOoe9wa7IyBNlbjEK-8F1AIpcMN4VR2tIGeWA0vdJ4PpvJA1R0N5Qn0zMsC4B6Y4megebgi3Jvq2BJ1g0jZRL9j9iYhPHFSdTUesgv9Q7p_PRb4dNNjnfnUGDeVxZHaCdWr-Iqatk6j8KcKYkusa9mKmGsz-2x_KF6xgU6Nx7QFE2yxAbegk8SnGIFiys8r99A; sess-at-main=\"NNbbNBJIIdO6ZUsf30V6vNV2aj2QETHGwAkzG3gX4ZE=\"; sst-main=Sst1|PQE1PjQcFvrt9Y3KNw8yA3eLEIpcrmUeovHZU8z9TftQ5cSjvbwa41EIkU6fNZnswQN4ItbnsOHE1dW6jzxtQ5W5bIW0nVNoX1SqhosV1IFUTgrfEJwe91NBnbMk4QMpfHZzjnBCtuP1l57JiSOetaBnrD4WZxe2IlQpTxhK57-mFsDAyjjPTpdWIsZFwK9KLFywkHaoJvVh6onccpXzMt_dEhnmwr3OeBWfFPagGojOYGpcnso44cDXzkLBMph3j_EnVk0ms-t-HRbuo64GAJ0O-Gc0e6o3D4JVCIy3pd9tERCyOn9iggvsICLQ5NDuq3IPop8KSk3xevMVZoEEucCO11ml7yWYg_Yo9wAlBGqRX9C4697s9qyM_llnAG0nn1KGTS-2BGyGl_7W47MtvpGTCtRdTp9jIJMFiwFPjUQ6h9to0Hligj45NYrF0p1JtCsH; i18n-prefs=USD; lc-main=en_US; aws-ubid-main=176-3363682-5411304; aws-session-id=140-0883594-3508442; aws-session-id-time=2232452694l; aws-session-token=\"4AcXH13Lb7gLaLAbVWxzhrnUWw5VZWqZi720/Dto7jgpDRcELFSp5rdL/70JOV/CzC3Q7NX5fwXr1hOyO1SGinrTZ2/mJwX9JI22zhbAj64paG31aDmWkELeUqOO9jxn87nBCHKuubVjDeLMfQoBykloAFSaSt6E/K+EiIG2dMyk5aC6G1Icjvrl1bdKmSGt3a+v4I/vPmozdi8jzhcihQnIA5aY3o5i/8p+kzEfCSQ=\"; sess-aws-at-main=\"OAo44oXc4/XY6EgTI5DzZjccXmr3Pf10LEqvTiATPBk=\"; regStatus=registered; aws_lang=en; s_fid=54929DF7E2EC5594-1DEC0D72EFDC6854; s_vn=1633268724773%26vn%3D1; s_invisit=true; s_cc=true; aws-target-visitor-id=1601732725193-893467; aws-target-data=%7B%22support%22%3A%221%22%7D; aws-userInfo=%7B%22arn%22%3A%22arn%3Aaws%3Aiam%3A%3A231071709544%3Aroot%22%2C%22alias%22%3A%22%22%2C%22username%22%3A%22jeff%2520james%22%2C%22keybase%22%3A%22%22%2C%22issuer%22%3A%22https%3A%2F%2Fwww.amazon.com%2Fap%2Fsignin%22%2C%22signinType%22%3A%22PUBLIC%22%7D; skin=noskin; csd-key=eyJ2IjoxLCJraWQiOiIzZTQwMDciLCJrZXkiOiJFTGpoTmpBeGhmNjgvTGdKQVNuTDBUcWx0a2JYaHRQdkVLRTRienk0bmVUM2Jnd0pHTHVlUlBjYkxya0RQVVRWMlV1YWI5WWZBMkZXTG93c1BxUUNtM21BL2RCbitJVkVuSmhFZkRqY2JUWEQrTllWbGlQSkhDeU9QYkpKa0RkYmNQRWtvOXNZa0VzU3hOZXFEbkZkUkk4b3FzY0dNZUU3MWFEVHcrRlNFVCtZYmd3amc3V0YyNjBoK29qQ0p6cVhkRDJzUVJBeVFtNmJxZG1TdXkzS092YkRIM293amNseTZVa0tKeGYzLzE5enA4OG10VWM2UG53clVvelQxMzdSbVpkVnlsZisvbjdSTGI0WkUzbjMzWDlEL1lsbmxhK0duLytlaU9qTjMzQUd3NG1NRC9oOTc0dFlqdmFQWnJDT2xKTk1RNmFVY3FpTDhReWQ0TUxwVFE9PSJ9; session-id-apay=143-6467005-0669008; session-id-time=2082787201l; session-id=146-2689202-9572416; s_dslv_s=More%20than%2030%20days; s_depth=2; s_dslv=1606267728256; s_nr=1606267728265-Repeat; session-token=\"gg4ozkVpSmo/CMUdJGU4vWV1Ap01LEGeSpGlQkj7ZEa3VEBeb+7xCHNDf8DV2y2tr45chYDEFySwu8cJx7Y1FN6QdAhBjINCqmSudE2ms/C0+61bcVE1sGzdOXLfxh57MgpZuzU1Xi/z3o8TsWfewFly/Kl6Aq0tKSWEPXT08CejwjX16Neh+Q00ofyScckwc/Qv/Q0oYIUysh3th6kRYg==\"; csm-hit=tb:RP2649Z9F69V9KV5A8BV+sa-738Q8AECSK2TQNV33QMY-2WTAQEBB75E5T1SDK1R6|1606489267955&adb:adblk_yes&t:1606489267955',\n",
    "}\n",
    "\n",
    "class amazon_product_review_scraper:\n",
    "    \n",
    "    def __init__(self, product_asin,amazon_site=\"amazon.com\", sleep_time=2, start_page=1, end_page=2):\n",
    "        \n",
    "        # url\n",
    "        self.url = \"https://www.\" + amazon_site + \"/product-reviews/\" + product_asin + \"?pageNumber={}\"\n",
    "        self.sleep_time = sleep_time\n",
    "        self.reviews_dict = {\"Date Info\":[], \"Country Info\":[], \"Name\":[], \"Review Title\":[], \"Content\":[], \"Rating\":[], \"Link\":[], \"Product Title\":[]}\n",
    "        \n",
    "        self.proxies = self.proxy_generator()        \n",
    "        self.max_try = 10\n",
    "        self.ua = ua.random\n",
    "        self.proxy = choice(self.proxies)\n",
    "        \n",
    "        self.start_page = start_page\n",
    "        if (end_page == None):\n",
    "            self.end_page = self.total_pages()-4\n",
    "        else:\n",
    "            self.end_page = min(end_page, self.total_pages())\n",
    "\n",
    "\n",
    "    def total_pages(self):\n",
    "        \n",
    "        response = self.request_wrapper(self.url.format(1))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        ## TODO if else        \n",
    "        content = soup.find(\"div\", {\"data-hook\": \"cr-filter-info-review-rating-count\"})\n",
    "        total_reviews = int(content.text.replace(\"\\n\",\"\").split()[3].replace(\",\",\"\"))\n",
    "    \n",
    "        #print (\"Total reviews (all pages): {}\".format(total_reviews), flush=True)\n",
    "        \n",
    "        total_pages = math.ceil(total_reviews/10)\n",
    "        return total_pages\n",
    "    \n",
    "\n",
    "    # page scrapper\n",
    "    def helper(self, content, tag, parameter_key, parameter_value):\n",
    "        attribute_lst = []\n",
    "        attributes = content.find_all(tag, {parameter_key: parameter_value})\n",
    "        for attribute in attributes:\n",
    "            attribute_lst.append(attribute.contents[0])\n",
    "        return attribute_lst\n",
    "\n",
    "    # MAIN FUNCTION\n",
    "    def scrape(self):\n",
    "\n",
    "        \n",
    "        #print (\"Total pages: {}\".format(self.end_page - self.start_page+1), flush=True)\n",
    "        #print (\"Start page: {}; End page: {}\".format(self.start_page, self.end_page))\n",
    "        #print ()\n",
    "        #print (\"Started!\", flush=True)\n",
    "\n",
    "        for page in tqdm(range(self.start_page, self.end_page+1)):\n",
    "            res = self.page_scraper(page)\n",
    "            # print(self.reviews_dict)\n",
    "            #\n",
    "            if res == None:\n",
    "                time.sleep(self.sleep_time)\n",
    "            else:\n",
    "                #print (\"Not able to scrape page {} Waiting for 10 sec more to bypassed CAPTCHA\".format(page))\n",
    "                time.sleep(5)\n",
    "                res = self.page_scraper(page)\n",
    "                \n",
    "\n",
    "\n",
    "        #print (\"Completed!\")\n",
    "        \n",
    "        # returning df\n",
    "        #print(len(self.reviews_dict[\"Date Info\"]))\n",
    "        #print(len(self.reviews_dict[\"Country Info\"]))\n",
    "        #print(len(self.reviews_dict[\"Name\"]))\n",
    "        #print(len(self.reviews_dict[\"Review Title\"]))\n",
    "        #print(len(self.reviews_dict[\"Content\"]))\n",
    "        #print(len(self.reviews_dict[\"Link\"]))\n",
    "        #print(len(self.reviews_dict[\"Product Title\"]))\n",
    "        #print(len(self.reviews_dict[\"Rating\"]))\n",
    "        \n",
    "        return self.reviews_dict\n",
    "\n",
    "\n",
    "\n",
    "    def page_scraper(self, page):\n",
    "        try:\n",
    "            response = self.request_wrapper(self.url.format(page))   \n",
    "            # parsing content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            ## reviews section\n",
    "            reviews = soup.findAll(\"div\", {\"class\": \"a-section review aok-relative\"})\n",
    "            ## parsing reviews section\n",
    "            reviews = BeautifulSoup('<br/>'.join([str(tag) for tag in reviews]), 'html.parser')\n",
    "            \n",
    "            ## 1. title\n",
    "            title_lst = []\n",
    "            try:\n",
    "                titles = reviews.find_all(\"a\", class_=\"review-title\") or reviews.find_all(\"a\", {\"data-hook\": \"review-title\"}) or reviews.find_all(\"span\", class_=\"review-title\")\n",
    "            except:\n",
    "                titles = []\n",
    "            \n",
    "            for title in titles:\n",
    "                try:\n",
    "                    title_lst.append(title.find_all(\"span\")[0].contents[0])\n",
    "                except:\n",
    "                    title_lst.append(\"\")\n",
    "            ## 2. name\n",
    "            \n",
    "            name_lst = self.helper(reviews, \"span\", \"class\", \"a-profile-name\")\n",
    "            \n",
    "            ## 3. rating\n",
    "            \n",
    "            rating_lst = []\n",
    "            try:\n",
    "                ratings = reviews.find_all(\"i\", {\"data-hook\": \"review-star-rating\"}) or reviews.find_all(\"i\", {\"data-hook\": \"cmps-review-star-rating\"})\n",
    "            \n",
    "            except:\n",
    "                ratings = []\n",
    "            \n",
    "            for rating in ratings:\n",
    "                try:\n",
    "                    rating_lst.append(rating.find_all(\"span\")[0].contents[0])\n",
    "                except:\n",
    "                    rating_lst.append(\"\")\n",
    "            \n",
    "            ## 4. date\n",
    "            date_lst = [i.split(\"on\")[1] for i in self.helper(reviews, \"span\", \"data-hook\", \"review-date\")]   \n",
    "            ## 5. country\n",
    "            country_lst = [i.split(\"on\")[0] for i in self.helper(reviews, \"span\", \"data-hook\", \"review-date\")]\n",
    "            \n",
    "            ## 6. content\n",
    "            contents = reviews.find_all(\"span\", {\"data-hook\": \"review-body\"})\n",
    "            content_lst = []\n",
    "            \n",
    "            for content in contents:\n",
    "                try:\n",
    "                    text_ = content.find_all(\"span\")[0].get_text(\"\\n\").strip()\n",
    "                    text_ = \". \".join(text_.splitlines())\n",
    "                    text_ = re.sub(' +', ' ', text_)\n",
    "                    content_lst.append(text_)\n",
    "                except:\n",
    "                    content_lst.append(\"\")\n",
    "            ## 7. Link\n",
    "            product_links = soup.find_all(\"a\", {\"data-hook\": \"product-link\"})\n",
    "            product_link = product_links[0][\"href\"] if product_links else \"\"\n",
    "            product_link = [product_link for i in range(int(len(content_lst)))]\n",
    "            \n",
    "            ## 8. title\n",
    "            product_title = product_links[0].text if product_links else \"\"\n",
    "            product_title = [product_title for i in range(int(len(content_lst)))]\n",
    "            \n",
    "            # adding to the main list\n",
    "            self.reviews_dict['Date Info'].extend(date_lst)\n",
    "            self.reviews_dict['Country Info'].extend(country_lst)\n",
    "            self.reviews_dict['Name'].extend(name_lst)\n",
    "            self.reviews_dict['Review Title'].extend(title_lst)\n",
    "            self.reviews_dict['Content'].extend(content_lst)\n",
    "            self.reviews_dict['Rating'].extend(rating_lst)\n",
    "            self.reviews_dict['Link'].extend(product_link)\n",
    "            self.reviews_dict['Product Title'].extend(product_title)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # wrapper around request package to make it resilient\n",
    "    def request_wrapper(self, url):\n",
    "        \n",
    "        while (True):\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "            # amazon blocks requests that does not come from browser, therefore need to mention user-agent\n",
    "            # response = requests.get(url, verify=False, headers={'User-Agent': self.ua}, proxies=self.proxy)\n",
    "            SESSION = requests.Session() \n",
    "            # print(url) \n",
    "            response = SESSION.get(url, headers=HEADERS)\n",
    "            # print(response.status_code)\n",
    "            # checking the response code\n",
    "            if (response.status_code != 200):\n",
    "                pass\n",
    "                # raise Exception(response.raise_for_status())\n",
    "            \n",
    "            # checking whether capcha is bypassed or not (status code is 200 in case it displays the capcha image)\n",
    "            if \"api-services-support@amazon.com\" in response.text:\n",
    "                \n",
    "                if (self.max_try == 0):\n",
    "                    raise Exception(\"CAPTCHA is not bypassed\")\n",
    "                else:\n",
    "                    time.sleep(self.sleep_time)\n",
    "                    self.max_try -= 1\n",
    "                    self.ua = ua.random\n",
    "                    self.proxy = choice(self.proxies)\n",
    "                    continue\n",
    "                \n",
    "            self.max_try = 5\n",
    "            break\n",
    "            \n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "    # random proxy generator\n",
    "    def proxy_generator(self):\n",
    "        proxies = []\n",
    "        response = requests.get(\"https://sslproxies.org/\")\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        proxys = pd.read_html(response.text)[0][[\"IP Address\",\"Port\"]]\n",
    "        proxies= list(proxys[\"IP Address\"].astype(str)+\":\"+proxys[\"Port\"].astype(str))\n",
    "        proxies_lst = [{'http':'http://'+proxy} for proxy in proxies]\n",
    "        return proxies_lst\n",
    "\n",
    "# helps scrape data \n",
    "def onehelper( content, tag, parameter_key, parameter_value):\n",
    "    attribute_lst = []\n",
    "    attributes = content.find_all(tag, {parameter_key: parameter_value})\n",
    "    for attribute in attributes:\n",
    "        attribute_lst.append(attribute.contents[0])\n",
    "    return attribute_lst\n",
    "\n",
    "# get ASIN number from keyword search\n",
    "def get_ASIN_LST(Key_word):\n",
    "    key_url=\"https://www.amazon.com/s?k=\"+str(Key_word)\n",
    "    #print(Key_word)\n",
    "\n",
    "    \n",
    "    s = requests.Session()            \n",
    "    response = s.get(key_url,headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    reviewsASIN = soup.findAll(\"div\", {\"data-component-type\":\"s-search-result\"})\n",
    "    \n",
    "    ASIN_lst=[]\n",
    "    for i in reviewsASIN:\n",
    "        ASIN_lst.append(i[\"data-asin\"])\n",
    "    # print(ASIN_lst)\n",
    "    return ASIN_lst\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "\n",
    "# ... (rest of your code)\n",
    "\n",
    "def main(ASIN, Path_name):\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # If running in a regular Python script, use the directory of the script\n",
    "    if '__file__' in locals():\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        current_dir = script_dir\n",
    "\n",
    "    CSV_PATH = os.path.join(current_dir, Path_name)\n",
    "    asinlst = get_ASIN_LST(ASIN)\n",
    "    final_df = []\n",
    "\n",
    "    for ASIN in asinlst:\n",
    "        scrap = amazon_product_review_scraper(product_asin=ASIN)\n",
    "        #print(ASIN)\n",
    "        reviews_df = scrap.scrape()\n",
    "        reviews_df = pd.DataFrame.from_dict(reviews_df, orient='index')\n",
    "        reviews_df = reviews_df.transpose()\n",
    "        final_df.append(reviews_df)\n",
    "\n",
    "    review_df_final = pd.concat(final_df)\n",
    "    review_df_final.to_csv('Amazon_Real_Time_Reviews_Prediction_Data.csv', index=False)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if running in IPython environment\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        # Check if in a Jupyter notebook, parse arguments manually or set default values\n",
    "        keyword = input(\"Enter the keyword to search: \")\n",
    "        print(keyword)\n",
    "        path = get_ipython().getoutput(\"echo {0}\".format('-p PATH'))[0] if '-p PATH' in sys.argv else 'default_path'\n",
    "    else:\n",
    "        # Parse command-line arguments using argparse\n",
    "        parser = ArgumentParser(description='Your script description',\n",
    "                                formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "        parser.add_argument('-k', '--keyword', type=str, help='Keyword to search', default='default_keyword')\n",
    "        parser.add_argument('-p', '--path', type=str, help='Path to save the output file', default='default_path')\n",
    "        args = parser.parse_args()\n",
    "        keyword = input(\"Enter the keyword to search: \")\n",
    "        print(keyword)\n",
    "        path = args.path\n",
    "\n",
    "    # Call the main function with the provided arguments\n",
    "    main(keyword, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110f025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631cfecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe047c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01299b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
